{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto NLP\n",
    "\n",
    "Existem muitas transformações de recursos que precisam ser feitas em dados de texto para levá-los a um ponto que os algoritmos de aprendizado de máquina possam entender. Felizmente, o Spark colocou os mais importantes em chamadas convenientes do Feature Transformer.\n",
    "\n",
    "Vamos examiná-los antes de entrar no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "<p> <a href=\"http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\"> Tokenização </a> é o processo de pegar um texto (como uma frase) e dividi-lo em termos individuais (geralmente palavras). Uma classe simples de <a href=\"api/scala/index.html#org.apache.spark.ml.feature.Tokenizer\"> Tokenizer </a> fornece essa funcionalidade. O exemplo abaixo mostra como dividir frases em sequências de palavras. </p>\n",
    "\n",
    "<p><a href=\"api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer\">RegexTokenizer</a>permite mais\n",
    "  tokenização avançada com base na correspondência de expressão regular (regex).\n",
    "Por padrão, o parâmetro &#8220;pattern&#8221; (regex, default: <code>\"\\\\s+\"</code>)é usado como delimitadores para dividir o texto de entrada.\n",
    " Alternativamente, os usuários podem definir parâmetros &#8220;gaps&#8221; para falso indicando o regex &#8220;pattern&#8221; denota\n",
    " &#8220;tokens&#8221; em vez de dividir as lacunas e encontrar todas as ocorrências correspondentes como o resultado da tokenização.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternativamente, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remoção Stop Words \n",
    "\n",
    "<p><a href=\"https://en.wikipedia.org/wiki/Stop_words\">Stop words</a>são palavras que\n",
    "deve ser excluído da entrada, normalmente porque as palavras aparecem\n",
    "freqüentemente e não carregam tanto significado.</p>\n",
    "\n",
    "<p><code>StopWordsRemover</code>toma como entrada uma sequência de strings (por exemplo, a saída\n",
    "de um <a href=\"ml-features.html#tokenizer\">Tokenizer</a>)e deixa cair toda a parada\n",
    "palavras das sequências de entrada. A lista de palavras irrelevantes é especificada por\n",
    "um <code>stopWords</code> parâmetro. Palavras de parada padrão para alguns idiomas são acessíveis\n",
    "chamando <code>StopWordsRemover.loadDefaultStopWords(language)</code>, para qual disponível\n",
    "opções são &#8220;danish&#8221;, &#8220;dutch&#8221;, &#8220;english&#8221;, &#8220;finnish&#8221;, &#8220;french&#8221;, &#8220;german&#8221;, &#8220;hungarian&#8221;, \n",
    "&#8220;italian&#8221;, &#8220;norwegian&#8221;, &#8220;portuguese&#8221;, &#8220;russian&#8221;, &#8220;spanish&#8221;, &#8220;swedish&#8221; and &#8220;turkish&#8221;. \n",
    "Um parâmetro booleano <code> caseSensitive </code> indica se as correspondências devem ser sensíveis a maiúsculas e minúsculas\n",
    "(falso por padrão).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Um n-grama é uma sequência de nn tokens (normalmente palavras) para algum inteiro nn. A classe NGram pode ser usada para transformar recursos de entrada em nn-gramas.\n",
    "\n",
    "<p><code>NGram</code>leva como entrada uma sequência de strings (por exemplo, a saída de um <a href=\"ml-features.html#tokenizer\">Tokenizer</a>).  O parâmetro <code> n </code> é usado para determinar o número de termos em cada $ n $ -grama. A saída consistirá em uma seqüência de $ n $ -gramas onde cada $ n $ -grama é representado por uma string delimitada por espaço de $ n $ palavras consecutivas. Se a sequência de entrada contiver menos de <code> n </code> strings, nenhuma saída será produzida.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "# Extrator de Feature\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tf-idf\">TF-IDF</h2>\n",
    "\n",
    "<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Freqüência de documento de freqüência inversa de termo (TF-IDF) </a>\n",
    "é um método de vetorização de recursos amplamente utilizado na mineração de texto para refletir a importância de um termo\n",
    "a um documento no corpus. Denote um termo por <code> $ t $ </code>, um documento por d e o corpus por D.\n",
    "A frequência do termo <code> $ TF (t, d) $ </code> é o número de vezes que o termo <code> $ t $ </code> aparece no documento <code> $ d $ </code>, enquanto\n",
    "frequência do documento <code> $ DF (t, D) $ </code> é o número de documentos que contém o termo <code> $ t $ </code>. Se nós apenas usarmos\n",
    "frequência de termos para medir a importância, é muito fácil enfatizar demais os termos que parecem muito\n",
    "frequentemente, mas trazem poucas informações sobre o documento, por exemplo, &#8220;a&#8221;, &#8220;the&#8221;, e &#8220;of&#8221;. Se um termo aparecer\n",
    "muitas vezes em todo o corpus, isso significa não traz informações especiais sobre um determinado documento.\n",
    "A frequência inversa do documento é uma medida numérica de quanta informação um termo fornece:\n",
    "\n",
    "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n",
    "\n",
    "onde | D | é o número total de documentos no corpus. Uma vez que o logaritmo é usado, se um termo\n",
    "aparece em todos os documentos, seu valor IDF torna-se 0. Observe que um termo de suavização é aplicado para evitar\n",
    "dividindo por zero para termos fora do corpus. A medida TF-IDF é simplesmente o produto de TF e IDF:\n",
    "\n",
    "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternativamente, CountVectorizer também pode ser usado para obter vetores de frequência de termo\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "CountVectorizer e CountVectorizerModel visam ajudar a converter uma coleção de documentos de texto em vetores de contagens de tokens. Quando um dicionário a priori não está disponível, CountVectorizer pode ser usado como um Estimator para extrair o vocabulário e gera um CountVectorizerModel. O modelo produz representações esparsas para os documentos sobre o vocabulário, que podem então ser passadas para outros algoritmos como LDA.\n",
    "\n",
    "Durante o processo de adaptação, CountVectorizer selecionará as principais palavras do vocabSize ordenadas por frequência de termo no corpus. Um parâmetro opcional minDF também afeta o processo de adaptação, especificando o número mínimo (ou fração se <1,0) de documentos em que um termo deve aparecer para ser incluído no vocabulário. Outro parâmetro de alternância binário opcional controla o vetor de saída. Se definido como verdadeiro, todas as contagens diferentes de zero são definidas como 1. Isso é especialmente útil para modelos probabilísticos discretos que modelam contagens binárias, em vez de inteiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Dados de entrada: cada linha é um pacote de palavras com um ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# ajustar um CountVectorizerModel do corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
