{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto\n",
    "\n",
    "Uma grande empresa de tecnologia precisa de sua ajuda, eles foram hackeados! Felizmente, os engenheiros forenses coletaram dados valiosos sobre os hacks, incluindo informações como tempo da sessão, locais, velocidade de digitação do wpm etc. A engenheira forense relata a você o que ela foi capaz de descobrir até agora, ela conseguiu obter meta dados de cada sessão que os hackers usaram para se conectar aos seus servidores. Estas são as características dos dados:\n",
    "\n",
    "* 'Session_Connection_Time': Quanto tempo a sessão durou em minutos\n",
    "* 'Bytes Transferidos': Número de MB transferidos durante a sessão\n",
    "* 'Kali_Trace_Used': indica se o hacker estava usando Kali Linux\n",
    "* 'Servers_Corrupted': Número de servidores corrompidos durante o ataque\n",
    "* 'Pages_Corrupted': Número de páginas acessadas ilegalmente\n",
    "* 'Local': O ataque ao local veio (provavelmente inútil porque os hackers usaram VPNs)\n",
    "* 'WPM_Typing_Speed': Sua velocidade de digitação estimada com base em logs de sessão.\n",
    "\n",
    "\n",
    "A empresa de tecnologia tem 3 hackers em potencial que perpetraram o ataque. Eles têm certeza dos dois primeiros hackers, mas não têm muita certeza se o terceiro hacker estava envolvido ou não. Eles pediram sua ajuda! Você pode ajudar a descobrir se o terceiro suspeito teve ou não algo a ver com os ataques ou foram apenas dois hackers? Provavelmente não é possível saber com certeza, mas talvez o que você acabou de aprender sobre Clustering possa ajudar!\n",
    "\n",
    "** Um último fato importante, o engenheiro forense sabe que os hackers compensam os ataques. O que significa que cada um deve ter aproximadamente a mesma quantidade de ataques. Por exemplo, se houvesse 100 ataques no total, em uma situação de 2 hackers, cada um deveria ter cerca de 50 hacks, em uma situação de três hackers cada um teria cerca de 33 hacks. O engenheiro acredita que esse é o elemento-chave para resolver isso, mas não sabe como distinguir esses dados não rotulados em grupos de hackers. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('hack_find').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Carregando dados.\n",
    "dataset = spark.read.csv(\"hack_data.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
    "             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = vec_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule estatísticas resumidas ajustando o StandardScaler\n",
    "scalerModel = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize cada recurso para ter um desvio padrão da unidade.\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hora de descobrir se são 2 ou 3! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com K=3\n",
      "------------------------------------------------------------\n",
      "Com K=2\n"
     ]
    }
   ],
   "source": [
    "print(\"Com K=3\")\n",
    "print('--'*30)\n",
    "print(\"Com K=2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há muito a ganhar com o WSSSE, afinal, esperaríamos que, conforme K aumenta, o WSSSE diminui. No entanto, poderíamos continuar a análise observando a queda de K = 3 para K = 4 para verificar se o agrupamento favorece números pares ou ímpares. Isso não será substancial, mas vale a pena dar uma olhada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=2\n",
      "------------------------------------------------------------\n",
      "With K=3\n",
      "------------------------------------------------------------\n",
      "With K=4\n",
      "------------------------------------------------------------\n",
      "With K=5\n",
      "------------------------------------------------------------\n",
      "With K=6\n",
      "------------------------------------------------------------\n",
      "With K=7\n",
      "------------------------------------------------------------\n",
      "With K=8\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "#    wssse = model.computeCost(cluster_final_data)\n",
    "    print(\"With K={}\".format(k))\n",
    "#    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "    print('--'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Nada definitivo pode ser dito com o acima, mas espere! O último fato importante que o engenheiro mencionou foi que os ataques deveriam ser numerados igualmente entre os hackers! Vamos verificar as colunas de transformação e predição que resultam disso! Parabéns, se você fez essa conexão, foi bem complicado, considerando o que abordamos! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   88|\n",
      "|         2|   79|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k3.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k2.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eram 2 hackers, na verdade, nosso algoritmo de clustering criou dois clusters do mesmo tamanho com K = 2, de jeito nenhum isso é uma coincidência!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
